Hive interview Questions

1. What is the definition of Hive? What is the present version of Hive? 
Ans1. Hive is a data warehouse system which is used to analyze structured data. It is built on the top of Hadoop. Its latest version is 4.0.0-alpha-2.

2. Is Hive suitable to be used for OLTP systems? Why?
Ans2. Hive doesn't support OLTP. Apache Hive is mainly used for batch processing i.e. OLAP and it is not used for OLTP because of the real-time operations of the database.

3. How is HIVE different from RDBMS? Does hive support ACID transactions. If not then give the proper reason.
A3. 
-RDBMS is used to maintain database, whereas Hive is used to maintain data warehouse.
-RDBMS uses SQL (Structured Query Language) whereas hive uses HQL (Hive Query Language).
-Schema is fixed in RDBMS whereas schema varies in Hive.
-In RDBMS, Normalized data is stored and in Hive-> Normalized and de-normalized both type of data is stored.
-RDBMS doesn’t support partitioning and Hive supports automation partition.
-Tables in rdms are sparse and Table in hive are dense.

Starting Version 0.14, Hive supports all ACID properties which enable us to use transactions, create transactional tables, and run queries like Insert, Update, and Delete on tables.

4. Explain the hive architecture and the different components of a Hive architecture?
A4. Hive Architecture step by step:-
Step-1: Execute Query – 
Interface of the Hive such as Command Line or Web user interface delivers query to the driver to execute. In this, UI calls the execute interface to the driver such as ODBC or JDBC. 
 
Step-2: Get Plan – 
Driver designs a session handle for the query and transfer the query to the compiler to make execution plan. In other words, driver interacts with the compiler. 
 
Step-3: Get Metadata – 
In this, the compiler transfers the metadata request to any database and the compiler gets the necessary metadata from the metastore. 
 
Step-4: Send Metadata – 
Metastore transfers metadata as an acknowledgment to the compiler. 
 
Step-5: Send Plan – 
Compiler communicating with driver with the execution plan made by the compiler to execute the query. 
 
Step-6: Execute Plan – 
Execute plan is sent to the execution engine by the driver. 
Execute Job
Job Done
Dfs operation (Metadata Operation)

Step-7: Fetch Results – 
Fetching results from the driver to the user interface (UI). 
 
Step-8: Send Results – 
Result is transferred to the execution engine from the driver. Sending results to Execution engine. 
When the result is retrieved from data nodes to the execution engine, it returns the result to the driver and to user interface (UI). 
-------------------------------------------------------------------------------------------------------------------------------------

The major components of Hive are described below: 

User Interface (UI) – 
As the name describes User interface provide an interface between user and hive. It enables user to submit queries and other operations to the system. Hive web UI, Hive command line, and Hive HD Insight (In windows server) are supported by the user interface. 
 
Hive Server – It is referred to as Apache Thrift Server. It accepts the request from different clients and provides it to Hive Driver.
Driver – 
Queries of the user after the interface are received by the driver within the Hive. Concept of session handles is implemented by driver. Execution and Fetching of APIs modelled on JDBC/ODBC interfaces is provided by the user. 
 
Compiler – 
Queries are parses, semantic analysis on the different query blocks and query expression is done by the compiler. Execution plan with the help of the table in the database and partition metadata observed from the metastore are generated by the compiler eventually. 

Metastore – 
All the structured data or information of the different tables and partition in the warehouse containing attributes and attributes level information are stored in the metastore. Sequences or de-sequences necessary to read and write data and the corresponding HDFS files where the data is stored. Hive selects corresponding database servers to stock the schema or Metadata of databases, tables, attributes in a table, data types of databases, and HDFS mapping. 
 
Execution Engine – 
Execution of the execution plan made by the compiler is performed in the execution engine. The plan is a DAG of stages. The dependencies within the various stages of the plan is managed by execution engine as well as it executes these stages on the suitable system components. 


5. Mention what Hive query processor does? And Mention what are the components of a Hive query processor?
A5. Hive query processor convert graph of MapReduce jobs with the execution time framework. So that the jobs can be executed in the order of dependencies.
    The major components of Apache Hive are the Hive clients, Hive services, Processing framework and Resource Management, and the Distributed Storage.

6. What are the three different modes in which we can operate Hive?
A6. Depending on the size of Hadoop data nodes, Hive can operate in two different modes:
    1.Local mode.
    2.Map-reduce mode.

7. Features and Limitations of Hive.
A7. 
Features:-
----------
1.Hive supports MapReduce, Tez, and Spark computing engine.
2.Hive is a stable batch-processing framework. It works as data warehouse.
3.Hive uses HIVE query language to query structure data which is easy to code.
4.HQL is a declarative language like SQL means it is non-procedural.
5.The table, the structure is similar to the RDBMS. It also supports partitioning and bucketing.
6.Partition, Bucket, and tables are the 3 data structures that hive supports.
7.It supports ETL.
8.Hive supports users to access files from HDFS, Apache HBase, Amazon S3, etc.
9.Hive is capable to process very large datasets of Petabytes in size.
10.We can easily embed custom MapReduce code with Hive to process unstructured data. 
11.Since we store Hive data on HDFS so fault tolerance is provided by Hadoop. 
12.JDBC/ODBC drivers are also available in Hive.
13.We can use a hive for data mining, predictive modeling, and document indexing.

Limitations:-
-------------
1.It doesn’t support online transaction processing (OLTP).
2.Subqueries are not supported.
3.The latency in the apache hive query is very high.
4.Hive is not used for real-time data querying since it takes a while to produce a result.
5.HQL does not support the Transaction processing feature.

8. How to create a Database in HIVE?
A8. Go to Hive shell by giving the command sudo hive and enter the command 'create database<data base name>' to create the new database in the Hive.

9. How to create a table in HIVE?
A9.
Managed table syntax==>

crate table <Table_name>
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '<delimiter>'
LINES TERMINATED BY '<delimiter>'
stored as '<fileFormat>'
--------------------------------
External table Syntax ===>

crate EXTERNAL table <Table_name>
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '<delimiter>' 
LINES TERMINATED BY '<delimiter>'
stored as '<fileFormat>'
LOCATION '<HDFS_locan>'   

10.What do you mean by describe and describe extended and describe formatted with respect to database and table?
A10.
-describe database shows/displays the name of the database, its comment and the root location on the file system & describe table shows/displays the list of columns (including partition columns) for the given table.
-describe extended - This will show table columns, data types, and other details of the table. Other details will be displayed in single line. 
-describe formatted - This will show table columns, data types, and other details of the table. Other details will be displayed into multiple lines. 

11.How to skip header rows from a table in Hive?
A11. While table creation, give this below syntax.
hive > tblproperties("skip.header.line.count"="1");

12.What is a hive operator? What are the different types of hive operators?
A12. 
-Hive provides various Built-in operators for data operations to be implemented on the tables present inside Apache Hive warehouse. Hive operators are used for mathematical operations on operands. It returns specific value as per the logic applied.
-Hive operators are as below:-
Operators Precedences
Relational Operators
Arithmetic Operators
Logical Operators
String Operators
Complex Type Constructors
Operators on Complex Types

13.Explain about the Hive Built-In Functions.
A13.
a. Collection Functions:
Basically, as par its name we use “Collection Functions” for collections.
Here, collections are nothing but defined as a grouping of elements and returning single or array of elements depends on return type mentioned in the function name.

ReturnType	Function Name		Description
INT	      size(Map<K.V>)	      It will fetch and give the components number in the map type
INT		size(Array<T>)		It will fetch and give the elements number in the array type
Array<K>	Map_keys(Map<K.V>)	It will fetch and gives an array containing the keys of the input map. Here array is in unordered
Array<V>	Map_values(Map<K.V>)	It will fetch and gives an array containing the values of the input map. Here array is in unordered
Array<t>	Sort_array(Array<T>)	sorts the input array in ascending order of array and elements and returns it

b. Hive Date Functions
However, to perform date manipulations and conversion of date types from one type to another type we use Hive date Functions. 
Each Date Function – Hive Built-in Functions in below table:

Function Name						Return Type			Description
Unix_Timestamp()						Unix_Timestamp()		We will get current Unix timestamp in seconds
To_date(string timestamp)				string			It will fetch and give the date part of a timestamp string:
year(string date)						INT				It will fetch and give the year part of a date or a timestamp string
quarter(date/timestamp/string)			INT				It will fetch and give the quarter of the year for a date, timestamp, or string in the range 1 to 4
month(string date)					INT				It will give the month part of a date or a timestamp string
hour(string date)						INT				It will fetch and gives the hour of the timestamp
minute(string date)					INT				It will fetch and gives the minute of the timestamp
Date_sub(string starting date, int days)		string			It will fetch and gives Subtraction of number of days to starting date
Current_date						date				It will fetch and gives the current date at the start of query evaluation
LAST _day(string date)					string			It will fetch and gives the last day of the month which the date belongs to
trunc(string date, string format)			string			It will fetch and gives date truncated to the unit specified by the format.
												Supported formats in this :MONTH/MON/MM, YEAR/YYYY/YY
c. Mathematical Functions
For Mathematical Operations in Hive, we use “Mathematical Functions”. Though, we have some inbuilt mathematical functions in Hive despite creating UDFs. 
Each Mathematical Function- Hive Built-in Functions in below table:

Function Name				Return 	Type	Description
round(DOUBLE X)				DOUBLE	It will fetch and returns the rounded BIGINT value of X
round(DOUBLE X, INT d)			DOUBLE	It will fetch and returns X rounded to d decimal places
bound(DOUBLE X)				DOUBLE	It will fetch and returns the rounded BIGINT value of X using HALF_EVEN rounding mode
floor(DOUBLE X)				BIGINT	It will fetch and returns the maximum BIGINT value that is equal to or less than X value
ceil(DOUBLE a), ceiling(DOUBLE a)	BIGINT	It will fetch and returns the minimum BIGINT value that is equal to or greater than X value
rand(), rand(INT seed)			DOUBLE	It will fetch and returns a random number that is distributed uniformly from 0 to 1

d. Conditional Functions
While it comes to conditional values checks in Hive, we use “Conditional Functions”. Each Conditional Function- Hive Built-In Functions in below table:

Function Name								Return Type		Description
if(Boolean testCondition, T valueTrue, T valueFalseOrNull)	T			It will fetch and gives value True when Test Condition is of true, gives value False Or Null otherwise.
ISNULL( X)									Boolean		It will fetch and gives true if X is NULL and false otherwise.
ISNOTNULL(X )								Boolean		It will fetch and gives true if X is not NULL and false otherwise.

e. Hive String Functions
For String manipulations and string operations in Hive, we call Hive String Functions. Each String Function- Hive Built-In Functions in below table:

Function Name						Return Type			Description
reverse(string X)						string			It will give the reversed string of X
rpad(string str, int length, string pad)		string			It will fetch and gives str, which is right-padded with the pad to a length of length(integer value)
rtrim(string X)						string			It will fetch and returns the string resulting from trimming spaces from the end (right-hand side) of X For example, 												rtrim(‘ results ‘) results in ‘ results’
space(INT n)						string			It will fetch and gives a string of n spaces.
split(STRING str, STRING pat)				array				Splits str around pat (pat is a regular expression).
Str_to_map(text[, delimiter1, delimiter2])	map<String ,String>	It will split text into key-value pairs using two delimiters.

14. Write hive DDL and DML commands.
A14.
- Hive DDL commands are the statements used for defining and changing the structure of a table or database in Hive. It is used to build or modify the tables and other objects in the database.The several types of Hive DDL commands are:
CREATE
SHOW
DESCRIBE
USE
DROP
ALTER
TRUNCATE

- Hive DML (Data Manipulation Language) commands are used to insert, update, retrieve, and delete data from the Hive table once the table and database schema has been defined using Hive DDL commands.The various Hive DML commands are:
LOAD
SELECT
INSERT
DELETE
UPDATE
EXPORT
IMPORT

15.Explain about SORT BY, ORDER BY, DISTRIBUTE BY and CLUSTER BY in Hive.
A15. The SORT BY and ORDER BY clauses are used to define the order of the output data whereas DISTRIBUTE BY and CLUSTER BY clauses are used to distribute the data to multiple reducers based on the key columns.

16.Difference between "Internal Table" and "External Table" and Mention when to choose “Internal Table” and “External Table” in Hive?
A.16
1. An internal table data is stored in the warehouse folder, whereas an external table data is stored at the location you mentioned in table creation.
2. Dropping an internal table deletes the table metadata from Metastore and also removes all its data/files from HDFS. 
   Dropping an external table, just drop the metadata of the table from Metastore and keeps the actual data as-is on HDFS location.
3. Create table tablename(---> for Internal table) & create EXTERNAL table tablename (---> for External table),Also we have to give the location.

-We create an external table for external use as when we want to use the data outside the Hive.
-Use internal tables when: The data is temporary.

17.Where does the data of a Hive table get stored?
A17.  Hive data are stored in one of Hadoop compatible filesystem: S3, HDFS or other compatible filesystem.
	Hive metadata are stored in RDBMS like MySQL, see supported RDBMS.

18.Is it possible to change the default location of a managed table?
A18. Yes, by using the clause – LOCATION '<hdfs_path>', we can change the default location of a managed table.

19.What is a metastore in Hive? What is the default database provided by Apache Hive for metastore?
A19.  Hive Metastore is a component in Hive that stores the catalog of the system that contains the metadata about Hive create columns, Hive table creation, and partitions. 
	Derby is the default database for the embedded metastore.

20.Why does Hive not store metadata information in HDFS?
A20.  Hive stores metadata information in the metastore using RDBMS instead of HDFS. 
	The reason for choosing RDBMS is to achieve low latency as HDFS read/write operations are time consuming processes.

21.What is a partition in Hive? And Why do we perform partitioning in Hive?
A21.  The partitioning in Hive means dividing the table into some parts based on the values of a particular column like date, course, city or country. 
	The advantage of partitioning is that since the data is stored in slices, the query response time becomes faster.

22.What is the difference between dynamic partitioning and static partitioning?
A22. 	Static  ===> We have to manually specify partition columns. Human errors are quite common.
	Dynamic ===> Instead of manually loading data into hive tables with partitions, we are instrucing hive to load data from basetable.

23.How do you check if a particular partition exists?
A23.SHOW PARTITIONS table_name 
    PARTITION(partitioned_column=’partition_value’)

24.How can you stop a partition form being queried?
A24.  By using the ENABLE OFFLINE clause with ALTER TABLE statement.

25.Why do we need buckets? How Hive distributes the rows into buckets?
A25. When there is more unique data(thats High Cardinality), we use bucketing.

26.In Hive, how can you enable buckets?
A26.partition=true property. So, we can enable dynamic bucketing while loading data into hive table By setting this property.

27.How does bucketing help in the faster execution of queries?
A27.Bucketing in Hive entails the decomposition of a table data set into smaller parts. Thus, data is easier to handle. With bucketing, we join similar data types and write them to a single file. This step here greatly enhances performance while joining tables or reading data.

28.How to optimise Hive Performance? Explain in very detail.
A28.

29. What is the use of Hcatalog?
A29.HCatalog is a tool that allows you to access Hive metastore tables within Pig, Spark SQL, and/or custom MapReduce applications.

30. Explain about the different types of join in Hive.
A30.
a. Inner Join
Basically, to combine and retrieve the records from multiple tables we use Hive Join clause. Moreover, in SQL JOIN is as same as OUTER JOIN. Moreover, by using the primary keys and foreign keys of the tables JOIN condition is to be raised.

b. Left Outer Join
On defining HiveQL Left Outer Join, even if there are no matches in the right table it returns all the rows from the left table.
To be more specific, even if the ON clause matches 0 (zero) records in the right table, then also this Hive JOIN still returns a row in the result. Although, it returns with NULL in each column from the right table.

c. Right Outer Join
Basically, even if there are no matches in the left table, HiveQL Right Outer Join returns all the rows from the right table.
To be more specific, even if the ON clause matches 0 (zero) records in the left table, then also this Hive JOIN still returns a row in the result. Although, it returns with NULL in each column from the left table
In addition, it returns all the values from the right table. Also, the matched values from the left table or NULL in case of no matching join predicate.

d. Full Outer Join
The major purpose of this HiveQL Full outer Join is it combines the records of both the left and the right outer tables which fulfills the Hive JOIN condition. Moreover, this joined table contains either all the records from both the tables or fills in NULL values for missing matches on either side.


31.Is it possible to create a Cartesian join between 2 tables, using Hive?
A31. No

32.Explain the SMB Join in Hive?
A32. SMB(Sort,Merge,Bucket) is a join performed on bucket tables that have the same sorted, bucket, and join condition columns. It reads data from both bucket tables and performs common joins (map and reduce triggered) on the bucket tables.

33.What is the difference between order by and sort by which one we should use?
A33.
Sort by:- hive> SELECT  E.EMP_ID FROM Employee E SORT BY E.empid;  
It may use multiple reducers for final output.
It only guarantees ordering of rows within a reducer.
It may give partially ordered result.

Order by:- hive> SELECT  E.EMP_ID FROM Employee E order BY E.empid;  
It uses single reducer to guarantee total order in output.
LIMIT can be used to minimize sort time.

34.What is the usefulness of the DISTRIBUTED BY clause in Hive?
A34. DISTRIBUTE BY clause is used to distribute the input rows among reducers. 
- It ensures that all rows for the same key columns are going to the same reducer. 

35.How does data transfer happen from HDFS to Hive? 
A35.
1.	Ingest the data. You create a single Sqoop import command that imports data from diverse data sources, such as a relational database, into HDFS.
2.	Convert the data to ORC format. 
3.	Incrementally update the imported data.

36.Wherever (Different Directory) I run the hive query, it creates a new metastore_db, please explain the reason for it?
A35. It creates the local metastore, while we run the hive in embedded mode. Also, it looks whether metastore already exist or not before creating the metastore.

37.What will happen in case you have not issued the command: ‘SET hive.enforce.bucketing=true;’ before bucketing a table in Hive?
A36. It may result into number of files that will be generated in the table directory to be not equal to the number of buckets.

38.Can a table be renamed in Hive?
A38. Yes By using ALTER command.

39.Write a query to insert a new column(new_col INT) into a hive table at a position before an existing column (x_col)
A39. ALTER TABLE table_name CHANGE COLUMN new_col INT BEFORE x_col 

40.What is serde operation in HIVE?
A40. A SerDe allows Hive to read in data from a table, and write it back out to HDFS in any custom format.
     Anyone can write their own SerDe for their own data formats.

41.Explain how Hive Deserializes and serialises the data?
A41. Hive uses the SerDe interface for IO. The interface handles both serialization and deserialization and also interpreting the results of serialization as individual fields for processing. A SerDe allows Hive to read in data from a table, and write it back out to HDFS in any custom format.

42.Write the name of the built-in serde in hive.
A42. 
-MetadataTypedColumnsetSerDe
-LazySimpleSerDe
-Thrift SerDe
-Dynamic SerDe

43.What is the need of custom Serde?
A43. It allows Hive to read in data from a table, and write it back out to HDFS in any custom format. Anyone can write their own SerDe for their own data formats. 

44.Can you write the name of a complex data type(collection data types) in Hive?
A44. Array, Map, Struct and union.

45.Can hive queries be executed from script files? How?
A45. Yes. We can execute Hive queries from the script files using the source command.

46.What are the default record and field delimiter used for hive text files?
A46. The default record delimiter is − \n
     And the filed delimiters are − \001,\002,\003

47.How do you list all databases in Hive whose name starts with s?
A47. SHOW DATABASES LIKE ‘s.*’

48.What is the difference between LIKE and RLIKE operators in Hive?
A48.We use LIKE to search for string with similar text. 
    RLIKE (Right-Like) is a special function in Hive where if any substring of A matches with B then it evaluates to true. 

49.How to change the column data type in Hive?
A49. By using ALTER command.
ALTER TABLE table_name CHANGE column_name column_name new_datatype;

50.How will you convert the string ’51.2’ to a float value in the particular column?
A50. By using the cast function to cast the type from one to another.
     CAST(from_datatype AS to_datatype)

51.What will be the result when you cast ‘abc’ (string) as INT?
A51. It will return NULL.

52.What does the following query do?
a. INSERT OVERWRITE TABLE employees
b. PARTITION (country, state)
c. SELECT ..., se.cnty, se.st
d. FROM staged_employees se;

A52. It will give all serialized country and serialized state and it is partition by only country and state from staged_employees serde table 

53.Write a query where you can overwrite data in a new table from the existing table.
A53.hive> INSERT OVERWRITE TABLE New_Table select * from Existing_Table;

54.What is the maximum size of a string data type supported by Hive? Explain how Hive supports binary formats.
A54. Max size=255
Hive supports two more primitive data types, BOOLEAN and BINARY.
Binary is a sequence of bytes. It is similar to the VARBINARY data type found in many relational databases.
	
55. What File Formats and Applications Does Hive Support?
A55. Hive supports File formats like comma-separated value (. csv) TextFile, RCFile, ORC, and Parquet and applications like Java,PHP,Python,C++,Ruby.

56.How do ORC format tables help Hive to enhance its performance?
A56.Using the ORC format leads to a reduction in the size of the data stored, as this file format has high compression ratios. As the data size is reduced, the time to read and write the data is also reduced. The ORC format improves query performance also by the way it stores data in a file.

57.How can Hive avoid mapreduce while processing the query?
A57. Hive avoid MapReduce to return query results by setting the hive.exec.mode.

58.What is view and indexing in hive?
A58.  Views are generated based on user requirements. You can save any result set data as a view.
	Indexes are a pointer or reference to a record in a table as in relational databases. Indexes facilitate in making query execution or search operation faster.

59.Can the name of a view be the same as the name of a hive table?
A59. The name of a view must be unique, and it cannot be the same as any table or database or view's name.

60.What types of costs are associated in creating indexes on hive tables?
A60.Basically, there is a processing cost in arranging the values of the column on which index is created since Indexes occupies.

61.Give the command to see the indexes on a table.
A61. To see the index for a specific table use SHOW INDEX: SHOW INDEX FROM table; 

62. Explain the process to access subdirectories recursively in Hive queries.
A62. We can use following commands in Hive to recursively access sub-directories:
hive> Set mapred.input.dir.recursive=true;
hive> Set hive.mapred.supports.subdirectories=true;
Once above options are set to true, Hive will recursively access sub-directories of a directory in MapReduce.

63.If you run a select * query in Hive, why doesn't it run MapReduce?
A63.Hive requires a map-reduce job since it needs to extract the 'column' from each row by parsing it from the file it loads.

64.What are the uses of Hive Explode?
A64. The explode function explodes an array to multiple rows. Returns a row-set with a single column (col), one row for each element from the array.

65. What is the available mechanism for connecting applications when we run Hive as a server?
A65.The mechanism is done by following the below steps:
Thrift client: By using thrift client, we can call Hive commands from different programming languages such as Java, Python, C++, Ruby
JDBC driver: It enables accessing data and supports Type 4 JDBC driver
ODBC driver: ODBC API Standards apply for the Hive DBMS. It supports ODBC protocols.

66.Can the default location of a managed table be changed in Hive?
A66.Yes, we can do it by using the clause – LOCATION '<hdfs_path>' we can change the default location of a managed table.

67.What is the Hive ObjectInspector function?
A67.Hive ObjectInspector is a group of flexible APIs to inspect value in different data representation

68.What is UDF in Hive?
A68. What is a UDF in Hive?
In Hive, the users can define own functions to meet certain client requirements. These are known as UDFs in Hive.

69.Write a query to extract data from hdfs to hive.
A69.
INSERT OVERWRITE DIRECTORY "HDFS_Path" ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' SELECT * FROM Table_name LIMIT 10;

70.What is TextInputFormat and SequenceFileInputFormat in hive.
A70. These are fifferent File FORMATS of MAPREDUCE.
     TextInputFormat ==>Default inputformat class

71.How can you prevent a large job from running for a long time in a hive?
A71. We can do it by setting the MapReduce jobs to execute in strict mode set hive.

72.When do we use explode in Hive?
A72.  Lateral View Explode is another function in Hive that is used to split a column, but instead of creating multiple rows, it creates multiple columns. 
	This function is beneficial when working with maps. 
	It allows us to split a map column into multiple columns, each containing one key-value pair from the map.

73.Can Hive process any type of data formats? Why? Explain in very detail
A73. Hive supports four file formats those are TEXTFILE, SEQUENCEFILE, ORC and RCFILE (Record Columnar File).
For single user metadata storage, Hive uses derby database and for multiple user Metadata or shared Metadata case Hive uses MYSQL.

74.Whenever we run a Hive query, a new metastore_db is created. Why?
A74. The property of interest here is javax.jdo.option.ConnectionURL. 
The default value of this property is jdbc:derby:;databaseName=metastore_db;create=true. 
This value specifies that you will be using embedded derby as your Hive metastore and the location of the metastore is metastore_db. 

75.Can we change the data type of a column in a hive table? Write a complete query.
A75. ALTER TABLE table_name CHANGE column_name column_name new_datatype;

76.While loading data into a hive table using the LOAD DATA clause, how do you specify it is a hdfs file and not a local file ?
A76.  hive> load data inpath 'HDFS_Path' into table table_name; ==>> This command load data from HDFS path
	hive> load data local inpath 'local_file_path' into table table_name; ==>> This command load data from LFS path

77.What is the precedence order in Hive configuration?
A77. We are using a precedence hierarchy for setting properties: The SET command in Hive. The command-line –hiveconf option.

78.Which interface is used for accessing the Hive metastore?
A78. WebHCat API web interface

79.Is it possible to compress json in the Hive external table ?
A79. Yes. We have to gzip the files and put them as is (*.gz) into the table location

80.What is the difference between local and remote metastores?
A80. 
Local Metastore:- Here metastore service still runs in the same JVM as Hive but it connects to a database running in a separate process either on same machine or on a remote machine.      Remote Metastore:- Metastore runs in its own separate JVM not on hive service JVM.

81.What is the purpose of archiving tables in Hive?
A81. To reduce the number of hdfs files in the Hive table partition.

82.What is DBPROPERTY in Hive?
A82. The DB properties are nothing but mentioning the details about the database created by the user.

83.Differentiate between local mode and MapReduce mode in Hive.
A83. Both MapReduce mode and local mode seem same to the user but the difference is the way they execute. 
